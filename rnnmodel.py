# -*- coding: utf-8 -*-
"""RNNModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MxuLEKLMSXAgE4_ZKXLCIHCGHC16ADCX
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd "/content/drive/My Drive/Tesina/20min"
!pwd

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plot
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
import time

x_Train = np.load('x_Train.npy', mmap_mode=None, allow_pickle=True, fix_imports=False)
y_Train = np.load('y_Train.npy', mmap_mode=None, allow_pickle=True, fix_imports=False)

x_Test = np.load('x_Test.npy', mmap_mode=None, allow_pickle=True, fix_imports=False)
y_Test = np.load('y_Test.npy', mmap_mode=None, allow_pickle=True, fix_imports=False)

x_Val = np.load('x_Val.npy', mmap_mode=None, allow_pickle=True, fix_imports=False)
y_Val = np.load('y_Val.npy', mmap_mode=None, allow_pickle=True, fix_imports=False)
print(x_Train.shape)
print(y_Train.shape)

model1 = tf.keras.models.Sequential()

model1.add(tf.keras.layers.GRU(units=64,input_shape=(x_Train.shape[1],x_Train.shape[2]),return_sequences=False))

# model1.add(tf.keras.layers.LayerNormalization())

model1.add(tf.keras.layers.Dropout(0.2))



#Decrease learning rate
optimizer = keras.optimizers.Adam(learning_rate=0.0001)
model1.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))
model1.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])

# show model1 summary
model1.summary()

model1.fit(x_Train,y_Train,epochs=30, validation_data=(x_Val,y_Val),batch_size=32)

plot.plot(model1.history.history['accuracy'])
plot.plot(model1.history.history['val_accuracy'])
plot.title('Model Accuracy')
plot.ylabel('Accuracy')
plot.xlabel('Epoch')
plot.legend(['Train Set', 'Validation Set'], loc='upper left')
plot.show()

plot.plot(model1.history.history['loss'])
plot.plot(model1.history.history['val_loss'])
plot.title('Model Loss')
plot.ylabel('Loss')
plot.xlabel('Epoch')
plot.legend(['Train Set', 'Validation Set'], loc='upper left')
plot.show()

print("Validation acc set at\t",model1.evaluate(x_Val,y_Val))
# print("Test acc set at\t",model1.evaluate(x_Test,y_Test))
model1_predictions = model1.predict(x_Test)
np.unique((model1_predictions.round() == y_Test),return_counts=True)

model2 = tf.keras.models.Sequential()

model2.add(tf.keras.layers.GRU(units=32,input_shape=(x_Train.shape[1],x_Train.shape[2]),return_sequences=True))
# model2.add(tf.keras.layers.LSTM(units=32,input_shape=(x_Train.shape[1],x_Train.shape[2]),return_sequences=True,kernel_regularizer=tf.keras.regularizers.l2(0.0001),recurrent_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001)))

# model2.add(tf.keras.layers.Dropout(0.1))
model2.add(tf.keras.layers.Dropout(0.2))
# model2.add(tf.keras.layers.LSTM(units=64)) # best experiment was using 0.2
model2.add(tf.keras.layers.GRU(units=64)) # best experiment was using 0.2
# model2.add(tf.keras.layers.Dropout(0.1)) # not useful


model2.add(tf.keras.layers.Dense(units=32,activation='relu'))
# model2.add(tf.keras.layers.Dense(units=32,activation='relu'))


model2.add(tf.keras.layers.Dense(units=1,activation='sigmoid')) # another experiment could be using sigmoid 
optimizer = keras.optimizers.Adam(learning_rate=0.0001)
model2.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy']) #using nadam just like stackoverflow

# show model2 summary
model2.summary()

model2.fit(x_Train,y_Train,epochs=60,batch_size=32,validation_data=(x_Val,y_Val))

model3 = tf.keras.models.Sequential()

model3.add(tf.keras.layers.GRU(units=64,input_shape=(x_Train.shape[1],x_Train.shape[2]),return_sequences=True))

# model3.add(tf.keras.layers.LayerNormalization())


model3.add(tf.keras.layers.GRU(units=64,return_sequences=True))
# model3.add(tf.keras.layers.Dropout(0.5))

model3.add(tf.keras.layers.GRU(units=64,recurrent_dropout=0.2)) # ,recurrent_dropout=0.2
model3.add(tf.keras.layers.Dropout(0.2))
model3.add(tf.keras.layers.Dense(units=128,activation='relu'))

model3.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))
model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

# show model3 summary
model3.summary()

model3.fit(x_Train,y_Train,epochs=60,batch_size=32,validation_data=(x_Val,y_Val))

plot.plot(model3.history.history['accuracy'])
plot.plot(model3.history.history['val_accuracy'])
plot.title('Model Accuracy')
plot.ylabel('Accuracy')
plot.xlabel('Epoch')
plot.legend(['Train Set', 'Validation Set'], loc='upper left')
plot.show()

plot.plot(model3.history.history['loss'])
plot.plot(model3.history.history['val_loss'])
plot.title('Model Loss')
plot.ylabel('Loss')
plot.xlabel('Epoch')
plot.legend(['Train Set', 'Validation Set'], loc='upper left')
plot.show()

print("Validation acc set at\t",model3.evaluate(x_Val,y_Val))
# print("Test acc set at\t",model1.evaluate(x_Test,y_Test))
print(x_Test.shape)
model3_predictions = model3.predict(x_Test)


np.unique((model3_predictions.round() == y_Test),return_counts=True)
# print((model3_predictions.round() == y_Test).shape)

